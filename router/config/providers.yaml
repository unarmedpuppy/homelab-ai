# Local AI Router - Provider and Model Configuration
# This file defines all available providers (hardware and cloud services)
# and the models they can serve.
#
# Environment Variable Overrides:
#   PROVIDER_{ID}_ENDPOINT - Override provider endpoint URL
#   PROVIDER_{ID}_ENABLED - Override provider enabled flag
#   PROVIDER_{ID}_API_KEY - Set API key for cloud providers

# ============================================================================
# PROVIDERS
# ============================================================================

providers:
  # Gaming PC with RTX 3090 - Escalation target for large context
  # Endpoint configured via PROVIDER_GAMING_PC_3090_ENDPOINT or GAMING_PC_URL env var
  - id: gaming-pc-3090
    name: "Gaming PC (RTX 3090)"
    type: local
    description: "Gaming PC with RTX 3090 - used for large context (>2K tokens) or explicit requests"
    endpoint: "${GAMING_PC_URL:-http://gaming-pc.local:8000}"
    priority: 2  # Secondary - escalate from 3070 when needed
    enabled: true

    # Concurrency Control
    maxConcurrent: 1  # Single request at a time (GPU limitation)

    # Health Check Configuration
    healthCheckInterval: 30  # seconds
    healthCheckTimeout: 5    # seconds
    healthCheckPath: "/v1/models"  # vLLM standard health check endpoint

    # Authentication (null for local vLLM)
    authType: null
    authSecret: null

    # Metadata
    gpu: "RTX 3090"
    vram: "24GB"
    location: "gaming-pc"
    powerWatts: 250  # Typical inference power draw (used for cost calculation)

  # Home Server with RTX 3070 - Default always-on backend
  - id: server-3070
    name: "Server (RTX 3070)"
    type: local
    description: "Home server with RTX 3070 - always-on default for requests under 2K tokens"
    endpoint: "http://local-ai-server:8000"
    priority: 1  # Primary - default for all requests
    enabled: true

    # Concurrency Control
    maxConcurrent: 1  # Single request at a time (GPU limitation)

    # Health Check Configuration
    healthCheckInterval: 30  # seconds
    healthCheckTimeout: 5    # seconds
    healthCheckPath: "/v1/models"  # vLLM standard health check endpoint

    # Authentication (null for local vLLM)
    authType: null
    authSecret: null

    # Metadata
    gpu: "RTX 3070"
    vram: "8GB"
    location: "home-server"
    powerWatts: 180

  # Cloud Provider - Z.ai (Coding Plan)
  - id: zai
    name: "Z.ai Coding Plan"
    type: cloud
    description: "Z.ai cloud API (GLM models) - Coding Plan subscription, unlimited usage"
    endpoint: "https://api.z.ai/api/coding/paas/v4"
    priority: 3  # Cloud fallback
    enabled: true

    # Concurrency Control
    maxConcurrent: 100  # Parallel requests OK (cloud service)

    # Health Check Configuration
    healthCheckInterval: 60  # seconds (less frequent for cloud)
    healthCheckTimeout: 10   # seconds (cloud may be slower)
    healthCheckPath: "/models"  # Use /models as health check (no /health endpoint)

    # Authentication
    authType: "bearer"
    authSecret: "ZAI_API_KEY"  # Environment variable name

    # Metadata
    location: "cloud"
    notes: "OpenAI-compatible API. No rate limits or cost concerns."

  # Claude Harness - Claude Max subscription via Claude Code CLI
  # type: local -> uses /v1/chat/completions path (OpenAI-compatible, like vLLM)
  - id: claude-harness
    name: "Claude Harness (Claude Max)"
    type: local
    description: "Claude Max subscription via Claude Code CLI wrapper - uses OAuth, no API key needed"
    endpoint: "http://host.docker.internal:8013"  # FastAPI service on host (via Docker's host-gateway)
    priority: 4  # Final fallback (same as old anthropic)
    enabled: true

    # Concurrency Control
    maxConcurrent: 5  # Claude CLI has rate limits, be conservative

    # Health Check Configuration
    healthCheckInterval: 60  # seconds
    healthCheckTimeout: 10   # seconds
    healthCheckPath: "/health"

    # Authentication - none needed (uses host's Claude OAuth tokens)
    authType: null
    authSecret: null

    # Metadata
    location: "local"  # Runs on server host
    notes: "Wraps 'claude -p' CLI. Requires Claude Code installed and authenticated on host."


# ============================================================================
# MODELS
# ============================================================================

models:
  # -------------------------------------------------------------------------
  # Gaming PC 3090 Models (24GB VRAM)
  # -------------------------------------------------------------------------

  - id: qwen2.5-14b-awq
    name: "Qwen 2.5 14B Instruct AWQ"
    providerId: gaming-pc-3090
    description: "Qwen 2.5 14B - use for large context or explicit 'big' requests"
    contextWindow: 32768
    maxTokens: 8192
    isDefault: false

    # Capabilities
    capabilities:
      streaming: true
      functionCalling: true
      vision: false
      jsonMode: true

    # Tags for filtering/discovery
    tags:
      - chat
      - reasoning
      - coding
      - large-context

  - id: qwen-image-edit
    name: "Qwen Image Edit 2509"
    providerId: gaming-pc-3090
    description: "Qwen multimodal image editing model"
    contextWindow: 8192
    maxTokens: 4096
    isDefault: false

    # Capabilities
    capabilities:
      streaming: true
      functionCalling: false
      vision: true
      imageEdit: true
      jsonMode: false

    # Tags
    tags:
      - multimodal
      - image-edit
      - vision

  # -------------------------------------------------------------------------
  # Server 3070 Models (8GB VRAM)
  # -------------------------------------------------------------------------

  - id: qwen2.5-7b-awq
    name: "Qwen 2.5 7B Instruct AWQ"
    providerId: server-3070
    description: "Default always-on model - escalates to 3090 if context > 2K tokens"
    contextWindow: 2048
    maxTokens: 1024
    isDefault: true

    # Capabilities
    capabilities:
      streaming: true
      functionCalling: true
      vision: false
      jsonMode: true

    # Tags
    tags:
      - chat
      - fast
      - general
      - always-on
      - default

  - id: llama3.1-8b-awq
    name: "Llama 3.1 8B Instruct AWQ"
    providerId: server-3070
    description: "Llama 3.1 8B with AWQ - switchable alternative"
    contextWindow: 8192
    maxTokens: 4096
    isDefault: false

    # Capabilities
    capabilities:
      streaming: true
      functionCalling: true
      vision: false
      jsonMode: true

    # Tags
    tags:
      - chat
      - alternative
      - general

  # -------------------------------------------------------------------------
  # Z.ai Cloud Models
  # -------------------------------------------------------------------------

  - id: glm-4.7
    name: "GLM-4.7"
    providerId: zai
    description: "GLM-4.7 - latest flagship model with thinking support"
    contextWindow: 128000
    maxTokens: 4096
    isDefault: true

    costPer1kTokens: 0.002

    capabilities:
      streaming: true
      functionCalling: true
      vision: false
      jsonMode: true

    tags:
      - chat
      - reasoning
      - cloud
      - large-context
      - default

  - id: glm-4.5-air
    name: "GLM-4.5 Air"
    providerId: zai
    description: "GLM-4.5 Air - fast general purpose model"
    contextWindow: 128000
    maxTokens: 4096
    isDefault: false

    costPer1kTokens: 0.001

    capabilities:
      streaming: true
      functionCalling: true
      vision: false
      jsonMode: true

    tags:
      - chat
      - fast
      - cloud
      - large-context

  - id: glm-4.6
    name: "GLM-4.6"
    providerId: zai
    description: "GLM-4.6 - balanced capability and speed"
    contextWindow: 128000
    maxTokens: 4096
    isDefault: false

    costPer1kTokens: 0.002

    capabilities:
      streaming: true
      functionCalling: true
      vision: false
      jsonMode: true

    tags:
      - chat
      - reasoning
      - cloud
      - large-context

  # -------------------------------------------------------------------------
  # Claude Models (via Claude Harness - Claude Max subscription)
  # -------------------------------------------------------------------------

  - id: claude-sonnet
    name: "Claude Sonnet (via Claude Max)"
    providerId: claude-harness
    description: "Claude Sonnet via Claude Code CLI - uses Claude Max subscription"
    contextWindow: 200000
    maxTokens: 8192
    isDefault: true

    # Cost: $0 (included in Claude Max subscription)
    costPer1kTokens: 0

    capabilities:
      streaming: true
      functionCalling: true
      vision: true
      jsonMode: true

    tags:
      - chat
      - reasoning
      - cloud
      - vision
      - large-context
      - premium

  - id: claude-opus-4
    name: "Claude Opus 4 (via Claude Max)"
    providerId: claude-harness
    description: "Claude Opus 4 via Claude Code CLI - most capable model"
    contextWindow: 200000
    maxTokens: 32000
    isDefault: false

    costPer1kTokens: 0

    capabilities:
      streaming: true
      functionCalling: true
      vision: true
      jsonMode: true

    tags:
      - chat
      - reasoning
      - cloud
      - vision
      - large-context
      - premium
      - opus


# ============================================================================
# CONFIGURATION OPTIONS
# ============================================================================

# Global Settings
settings:
  # Cloud Fallback Control
  enableCloudFallback: true  # If false, queue locally or return 503

  # Queue Settings (when local-only mode and GPUs busy)
  queueTimeout: 5  # seconds
  queueMaxSize: 10  # Maximum queued requests

  # Request Priority
  agentPriority: 0  # High priority for agent requests
  userPriority: 1   # Normal priority for user requests

  # Warm/Cold Model Preference
  preferWarmModels: true  # Prefer warm models for local providers

  # Cost Tracking - Electricity Rate
  # Used to calculate cost for local GPU inference
  # Formula: (power_kw) × (duration_hours) × (rate_per_kwh) = cost_usd
  electricityRateKwh: 0.166  # $/kWh - update based on your utility rate
