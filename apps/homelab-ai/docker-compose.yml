# Homelab AI Stack - Server Deployment
# 
# Pulls pre-built images from Harbor registry. Source code lives in homelab-ai repo.
# 
# Components:
#   - llm-router: OpenAI-compatible API router with intelligent backend selection
#   - dashboard: React metrics dashboard with conversation explorer
#   - llm-manager: Unified LLM orchestrator for RTX 3070 (always-on mode)
#   - claude-harness: Claude Code CLI wrapper (built locally, needs OAuth tokens)
#
# Usage:
#   docker compose -f apps/homelab-ai/docker-compose.yml up -d
#
# For source code and development, see: https://github.com/unarmedpuppy/homelab-ai

services:
  llm-router:
    image: harbor.server.unarmedpuppy.com/library/llm-router:latest
    container_name: llm-router
    restart: unless-stopped
    ports:
      - "8012:8000"
    environment:
      - TZ=America/Chicago
      - LOCAL_3070_URL=http://llm-manager:8000
      - GAMING_PC_URL=${GAMING_PC_URL:-http://192.168.86.63:8000}
      - ENABLE_METRICS=1
      - ENABLE_MEMORY=1
      - LOG_LEVEL=INFO
      - ZAI_API_KEY=${ZAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
    volumes:
      - ./data:/data
      - ../../agents/skills:/app/agents/skills:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      my-network:
        aliases:
          - local-ai-router  # Backwards compat for dashboard image
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.llm-router.rule=Host(`homelab-ai-api.server.unarmedpuppy.com`)"
      - "traefik.http.routers.llm-router.entrypoints=websecure"
      - "traefik.http.routers.llm-router.tls.certresolver=myresolver"
      - "traefik.http.routers.llm-router.middlewares=llm-router-cors"
      - "traefik.http.services.llm-router.loadbalancer.server.port=8000"
      - "traefik.http.middlewares.llm-router-cors.headers.accesscontrolallowmethods=GET,POST,PUT,DELETE,OPTIONS"
      - "traefik.http.middlewares.llm-router-cors.headers.accesscontrolallowheaders=Content-Type,Authorization"
      - "traefik.http.middlewares.llm-router-cors.headers.accesscontrolalloworiginlist=https://homelab-ai.server.unarmedpuppy.com"
      - "traefik.http.middlewares.llm-router-cors.headers.accesscontrolmaxage=100"
      - "traefik.http.middlewares.llm-router-cors.headers.addvaryheader=true"
      - "homepage.group=AI"
      - "homepage.name=LLM Router"
      - "homepage.icon=mdi-api"
      - "homepage.href=https://homelab-ai-api.server.unarmedpuppy.com/health"
      - "homepage.description=OpenAI-compatible API router"

  dashboard:
    image: harbor.server.unarmedpuppy.com/library/local-ai-dashboard:latest
    container_name: homelab-ai-dashboard
    restart: unless-stopped
    ports:
      - "8014:80"
    environment:
      - VITE_API_URL=http://llm-router:8000
      - LOCAL_AI_API_KEY=${LLM_ROUTER_API_KEY:-}
    networks:
      - my-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ai-dashboard.rule=Host(`homelab-ai.server.unarmedpuppy.com`)"
      - "traefik.http.routers.ai-dashboard.entrypoints=websecure"
      - "traefik.http.routers.ai-dashboard.tls.certresolver=myresolver"
      - "traefik.http.services.ai-dashboard.loadbalancer.server.port=80"
      - "homepage.group=AI"
      - "homepage.name=AI Dashboard"
      - "homepage.icon=mdi-chart-line"
      - "homepage.href=https://homelab-ai.server.unarmedpuppy.com"
      - "homepage.description=Metrics and conversation explorer"

  llm-manager:
    image: harbor.server.unarmedpuppy.com/library/llm-manager:latest
    container_name: llm-manager
    restart: unless-stopped
    ports:
      - "8015:8000"
    environment:
      - MODE=always-on
      - DEFAULT_MODEL=qwen2.5-7b-awq
      - GAMING_MODE_ENABLED=false
      - IDLE_TIMEOUT=0
      - VLLM_IMAGE=harbor.server.unarmedpuppy.com/docker-hub/vllm/vllm-openai:v0.4.0
      - DOCKER_NETWORK=my-network
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - huggingface-cache:/root/.cache/huggingface
    networks:
      - my-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    labels:
      - "homepage.group=AI"
      - "homepage.name=LLM Manager"
      - "homepage.icon=mdi-chip"
      - "homepage.href=http://192.168.86.47:8015/status"
      - "homepage.description=Always-on LLM on RTX 3070"

  claude-harness:
    build:
      context: ../claude-harness
      args:
        USER_ID: 1000
        GROUP_ID: 1000
    container_name: claude-harness
    restart: unless-stopped
    ports:
      - "8013:8013"
    volumes:
      - /home/unarmedpuppy/.claude.json:/home/appuser/.claude.json:rw
      - /home/unarmedpuppy/.claude:/home/appuser/.claude:rw
    environment:
      - TZ=America/Chicago
    networks:
      - my-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8013/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    labels:
      - "homepage.group=AI"
      - "homepage.name=Claude Harness"
      - "homepage.icon=mdi-robot"
      - "homepage.href=http://192.168.86.47:8013/health"
      - "homepage.description=Claude Code CLI API wrapper"

networks:
  my-network:
    external: true

volumes:
  huggingface-cache:
    external: true
