# Homelab AI Stack - Server Deployment
# 
# Pulls pre-built images from Harbor registry. Source code lives in homelab-ai repo.
# 
# Components:
#   - llm-router: OpenAI-compatible API router with intelligent backend selection
#   - dashboard: React metrics dashboard with conversation explorer
#   - llm-manager: Unified LLM orchestrator for RTX 3070 (always-on mode)
#
# Usage:
#   docker compose -f apps/homelab-ai/docker-compose.yml up -d
#
# For source code and development, see: https://github.com/unarmedpuppy/homelab-ai

services:
  llm-router:
    image: harbor.server.unarmedpuppy.com/library/llm-router:latest
    container_name: llm-router
    restart: unless-stopped
    ports:
      - "8012:8000"
    environment:
      - LOCAL_3070_URL=http://llm-manager:8000
      - ENABLE_METRICS=1
      - ENABLE_MEMORY=1
      - LOG_LEVEL=INFO
    volumes:
      - router-data:/app/data
    networks:
      - my-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.llm-router.rule=Host(`local-ai-api.server.unarmedpuppy.com`)"
      - "traefik.http.routers.llm-router.entrypoints=websecure"
      - "traefik.http.routers.llm-router.tls.certresolver=myresolver"
      - "traefik.http.services.llm-router.loadbalancer.server.port=8000"
      - "homepage.group=AI"
      - "homepage.name=LLM Router"
      - "homepage.icon=mdi-api"
      - "homepage.href=https://local-ai-api.server.unarmedpuppy.com/health"
      - "homepage.description=OpenAI-compatible API router"

  dashboard:
    image: harbor.server.unarmedpuppy.com/library/local-ai-dashboard:latest
    container_name: local-ai-dashboard
    restart: unless-stopped
    ports:
      - "8014:80"
    environment:
      - VITE_API_URL=http://llm-router:8000
    networks:
      - my-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ai-dashboard.rule=Host(`local-ai-dashboard.server.unarmedpuppy.com`)"
      - "traefik.http.routers.ai-dashboard.entrypoints=websecure"
      - "traefik.http.routers.ai-dashboard.tls.certresolver=myresolver"
      - "traefik.http.services.ai-dashboard.loadbalancer.server.port=80"
      - "homepage.group=AI"
      - "homepage.name=AI Dashboard"
      - "homepage.icon=mdi-chart-line"
      - "homepage.href=https://local-ai-dashboard.server.unarmedpuppy.com"
      - "homepage.description=Metrics and conversation explorer"

  llm-manager:
    image: harbor.server.unarmedpuppy.com/library/llm-manager:latest
    container_name: llm-manager
    restart: unless-stopped
    ports:
      - "8015:8000"
    environment:
      - MODE=always-on
      - DEFAULT_MODEL=qwen2.5-7b-awq
      - GAMING_MODE_ENABLED=false
      - IDLE_TIMEOUT=0
      - VLLM_IMAGE=harbor.server.unarmedpuppy.com/docker-hub/vllm/vllm-openai:latest
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - huggingface-cache:/root/.cache/huggingface
    networks:
      - my-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    labels:
      - "homepage.group=AI"
      - "homepage.name=LLM Manager"
      - "homepage.icon=mdi-chip"
      - "homepage.href=http://192.168.86.47:8015/status"
      - "homepage.description=Always-on LLM on RTX 3070"

networks:
  my-network:
    external: true

volumes:
  huggingface-cache:
    external: true
  router-data:
    external: true
