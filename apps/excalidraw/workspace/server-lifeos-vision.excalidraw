{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.server.unarmedpuppy.com",
  "elements": [
    {
      "id": "MwKMD25G4MQL9SVEvGGSO",
      "type": "rectangle",
      "x": 567,
      "y": 235,
      "width": 164,
      "height": 99,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "a0",
      "roundness": {
        "type": 3
      },
      "seed": 1246882565,
      "version": 58,
      "versionNonce": 1305879621,
      "isDeleted": true,
      "boundElements": null,
      "updated": 1766247392827,
      "link": null,
      "locked": false
    },
    {
      "id": "HsNRM0g3aohYSrDABzfkv",
      "type": "text",
      "x": 609,
      "y": 339,
      "width": 1636.578857421875,
      "height": 425,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "a1",
      "roundness": null,
      "seed": 1809376555,
      "version": 1179,
      "versionNonce": 264844581,
      "isDeleted": false,
      "boundElements": null,
      "updated": 1766247840585,
      "link": null,
      "locked": false,
      "text": "Server\n- lifeOS\n- LLM routing service and light inference (3070)\n- can call into windows PC for heavy inference\n- Beads project/tasks lives on server, local agent can reference beads there.\n(when working locally, how can I have local agents reference same beads source of truth?)\n- lifeos persistence - avoid public git, consider gitea for local source management? lives on laptop, \nsyncs with separate repo that's deployed to server, included in server backups (todo)\n- this gives me: personal assistant using local inference, with persistent private data that is\nbacked up, as well as coding agents that can work on my server projects \nusing local inference (perhaps cloud agents if i can install opencode on server?)\n- how is personal assistant different that normal agents? its not. just different entry point i suppose, need opencode running either way - hooked up to local inference\n- have optional cloud inference available, API keys?\n- how does beads vs life todos work? separate beads projects or human readable stuff, do we want integration with obsidian? maybe\n- \n\n",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "left",
      "verticalAlign": "top",
      "containerId": null,
      "originalText": "Server\n- lifeOS\n- LLM routing service and light inference (3070)\n- can call into windows PC for heavy inference\n- Beads project/tasks lives on server, local agent can reference beads there.\n(when working locally, how can I have local agents reference same beads source of truth?)\n- lifeos persistence - avoid public git, consider gitea for local source management? lives on laptop, \nsyncs with separate repo that's deployed to server, included in server backups (todo)\n- this gives me: personal assistant using local inference, with persistent private data that is\nbacked up, as well as coding agents that can work on my server projects \nusing local inference (perhaps cloud agents if i can install opencode on server?)\n- how is personal assistant different that normal agents? its not. just different entry point i suppose, need opencode running either way - hooked up to local inference\n- have optional cloud inference available, API keys?\n- how does beads vs life todos work? separate beads projects or human readable stuff, do we want integration with obsidian? maybe\n- \n\n",
      "autoResize": true,
      "lineHeight": 1.25
    }
  ],
  "appState": {
    "gridSize": 20,
    "gridStep": 5,
    "gridModeEnabled": false,
    "viewBackgroundColor": "#ffffff",
    "lockedMultiSelections": {}
  },
  "files": {}
}