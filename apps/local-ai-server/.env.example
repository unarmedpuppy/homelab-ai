# Local AI Server Environment Variables

# HuggingFace token (optional - only if model requires authentication)
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxx

# Model configuration (change these to switch models)
# Default: Qwen2.5-7B-Instruct-AWQ
MODEL_NAME=Qwen/Qwen2.5-7B-Instruct-AWQ
SERVED_MODEL_NAME=qwen2.5-7b-awq

# Alternative: Llama 3.1 8B AWQ
# MODEL_NAME=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
# SERVED_MODEL_NAME=llama3.1-8b-awq

# GPU memory settings (adjust if OOM errors)
GPU_MEMORY_UTILIZATION=0.85
MAX_MODEL_LEN=8192
