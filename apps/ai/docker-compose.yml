version: '3.8'

services:
  deepseek-model:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: deepseek-model
    restart: unless-stopped
    ports:
      - "8042:8000"  # Model API exposed on port 8000
    environment:
      - MODEL_ID=deepseek-ai/DeepSeek-R1
      - QUANTIZE=gptq  # Enables quantization for better performance
      - HUGGING_FACE_HUB_TOKEN=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
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./models:/models  # Store downloaded models

  textgen:
    image: atinoda/text-generation-webui:latest
    container_name: textgen-webui
    restart: unless-stopped
    ports:
      - "7860:7860"  # WebUI Port
    volumes:
      - ./data:/data  # Model storage
      - ./characters:/characters  # Character presets
      - ./extensions:/extensions  # Extensions directory
      - ./presets:/presets  # Chat presets
      - ./loras:/loras  # Lora models
      - ./models:/models  # Custom models
    environment:
      - CLI_ARGS=--listen --api --model deepseek-ai/DeepSeek-R1 --backend text-generation-inference
    depends_on:
      - deepseek-model
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    labels:
      - "homepage.group=Apps"
      - "homepage.name=AI"
      - "homepage.icon=si-openai"
      - "homepage.href=http://192.168.86.47:7860/"
      - "homepage.description=LLM"
    networks:
      - my-network

networks:
  my-network:
    external: true